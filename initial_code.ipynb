{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfcdf09c-255c-431c-9632-7d7a3dc56ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# load machine learning packages\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# # minisom package for SOM  \n",
    "# import minisom\n",
    "# from minisom import MiniSom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1d0f6-e2e6-4cfe-8771-7295d7d400b5",
   "metadata": {},
   "source": [
    "# Importing data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab63d5c-c924-4646-b641-e3baa0eca610",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/storage/group/gsj1/default/COMMON/DATA/'\n",
    "dust_df = pd.read_csv(path+'saharan_dust_met_vars.csv', index_col='time')\n",
    "\n",
    "# print out shape of data \n",
    "print('Shape of data:', np.shape(dust_df))\n",
    "\n",
    "# print first 5 rows of data\n",
    "print(dust_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7e564-a818-4b01-bc29-7265085e1434",
   "metadata": {},
   "source": [
    "# Scaling the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53922952-e823-455c-91a9-48d286eab831",
   "metadata": {},
   "source": [
    "As you can see there is a large range of values bewteen variables. To not influence the results as it is the case in many unsupervised machine learning models, it is important to scale them. Many scaling methods exist but I am using the minmax scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff19e0f-85db-42d2-b452-0ccbf4e869ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler(feature_range = (0,1))\n",
    "sc.fit(dust_df)\n",
    "scaled_dust_df=sc.transform(dust_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226538c5-64b6-4194-bd32-22fc9590b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minisom model\n",
    "som = MiniSom(x=20, # map size\n",
    "              y=20, # map size, NxN\n",
    "              input_len=10, # 10 element input vectors\n",
    "              sigma=1.0,\n",
    "              learning_rate=0.5, \n",
    "              neighborhood_function='triangle' # a few options for this\n",
    "             )\n",
    "\n",
    "# input_len: number of features used for training the model\n",
    "# sigma: is the radius of the different neighbors in the SOM\n",
    "# learning rate: determines how much the weights are adjusted during each iterations\n",
    "\n",
    "# initilize weights using PCA\n",
    "# You could also do that using random_weights_init, but the advantage is that PCA is likely to converge faster\n",
    "#som.pca_weights_init(scaled_dust_df)  # prefrerred\n",
    "som.random_weights_init(scaled_dust_df)\n",
    "\n",
    "## training the SOM : there are two type of training\n",
    "# 1. train_random: trains model by pickinhg random data from the data\n",
    "# 2. train_batch: trains model from samples in the order in which they are fed.\n",
    "\n",
    "som.train(data = scaled_dust_df, num_iteration = 3000, \n",
    "          random_order=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b4064-1f6a-4f4c-8f77-8a96d1689e87",
   "metadata": {},
   "source": [
    "# Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e174184e-7175-490c-8bd3-c155198f9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import bone, pcolor, colorbar, plot, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33189826-386b-4812-82ad-47cdc0fbd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "bone()\n",
    "pcolor(som.distance_map().T)\n",
    "colorbar()\n",
    "\n",
    "# markers = ['o', 's']\n",
    "# colors = ['r', 'g']\n",
    "\n",
    "# for i, x in enumerate(scaled_dust_df):\n",
    "#     w = som.winner(x)\n",
    "#     # w[0], w[1] will place the marker at bottom left corner of the rectangle. \n",
    "#     #Let us add 0.5 to both of these to plot the market at the center of the rectange.\n",
    "#     plot(w[0] + 0.5, \n",
    "#          w[1] + 0.5,\n",
    "#          #Target value 0 will have marker \"o\" with color \"r\"\n",
    "#          #Target value 1 will have marker \"s\" with color \"g\"\n",
    "#          marker='o', \n",
    "#          markeredgecolor = 'r',\n",
    "#          markerfacecolor = 'None', #No color fill inside markers\n",
    "#          markersize = 10,\n",
    "#          markeredgewidth = 2)\n",
    "# show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0bca1-2d78-4353-a783-1c151394f100",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a64ee-d11e-4fdb-980d-b38566e8df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Model evaluation\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd0e87-5771-4ed6-993e-74b7c42576bb",
   "metadata": {},
   "source": [
    "# Define a function for error calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d65f3-7328-4ebe-ad72-eb82a0ae817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_stats(model_fit, model_name, features_test, pred, target_test):\n",
    "  #Calculate and display model error\n",
    "  score = model_fit.score(features_test,target_test)\n",
    "  print('\\n'+model_name)\n",
    "  print(f'Score : {score}')\n",
    "  print(f'MAE: {mean_absolute_error(pred,target_test)}')\n",
    "  print(f'RMSE : {np.sqrt(mean_squared_error(pred,target_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25ead8-7ebc-4524-a6da-cc6604592b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vars = dust_df['PM10']   # PM10 concentration is the target variable\n",
    "features = dust_df.drop(['PM10'], axis=1)  # remaining variables should be features\n",
    "\n",
    "# split the data into 70% training and reserve 30% for testing\n",
    "train_features, test_features, train_target, test_target = train_test_split(features, \n",
    "                                                                target_vars, test_size = 0.3)\n",
    "\n",
    "# Now train the model using the training sets\n",
    "rf = RandomForestRegressor(n_estimators=200,\n",
    "                    random_state=42, n_jobs=-1)\n",
    "# Fit Random forest model\n",
    "rf.fit(train_features, train_target)\n",
    "predictions = rf.predict(test_features)\n",
    "\n",
    "regression_stats(rf,'Random Forest',test_features, predictions, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628598ea-f3df-441e-8a61-ae3583f358f1",
   "metadata": {},
   "source": [
    "# Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b5e43-6d3b-4909-a9cd-d04be9412ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize figure \n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 28\n",
    "mpl.rcParams['legend.fontsize'] = 'large'\n",
    "mpl.rcParams['figure.titlesize'] = 'large'\n",
    "mpl.rcParams['lines.linewidth'] = 2.5\n",
    "mpl.rcParams['axes.linewidth'] = 2.5\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = True\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "mpl.rcParams['savefig.bbox']='tight'\n",
    "mpl.rcParams['hatch.linewidth'] = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5a75c-3dde-4b85-b9f3-98210eb8d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = list(features.columns)\n",
    "cols = np.array(['red', 'blue', 'green', 'cyan', 'pink', 'olive', 'purple', 'magenta',\n",
    "                'indigo'])\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(22,14), sharex=False, sharey=False, \n",
    "                               constrained_layout=True) \n",
    "\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "importances = rf.feature_importances_\n",
    "x_values = col_names\n",
    "\n",
    "# Make a bar chart\n",
    "ax.barh(features.columns[sorted_idx], \n",
    "           importances[sorted_idx], color=cols)\n",
    "\n",
    "ax.set_xlabel('Random Forest Feature Importance')     \n",
    "ax.set_ylabel('Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b552c9-48c5-4aa6-8aa3-6967e15de1a6",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512429e9-5129-4f9f-aef0-2eee3cb4fd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
